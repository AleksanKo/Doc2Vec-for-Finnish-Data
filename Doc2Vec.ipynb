{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "\n",
    "The data consists of 356 documents (paragraphs) which have one or more events.\n",
    "The data was collected from the BiographySampo.\n",
    "The first training corpus wasn't lemmatized.\n",
    "Currently used corpus is lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\konovale\\R kansio\\METH&BS\\new_corpus.csv') as corpus:\n",
    "    reader = corpus.readlines()\n",
    "    mydata = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging data, creating a model and training it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(mydata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so far the best results for test sentences are with this number of epochs and this vector size: 30 and 17\n",
    "max_epochs = 30\n",
    "vec_size = 17\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "    \n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents for testing\n",
    "\n",
    "As an example, such event as marriage was taken: \n",
    "\n",
    "1.\t\"Hän avioitui Pietarissa 2. toukokuuta 1892 venäläisen kenraalimajurin varakkaan perijättären Anastasia Arapovan kanssa.\"\n",
    "2.\t\"Lönnrot meni naimisiin vuonna 1849, 47-vuotiaana, Maria Piponiuksen kanssa, joka oli oululaisen värjärimestarin Elias Piponiuksen tytär.\"\n",
    "3.\t\"Tytär Karin solmi avioliiton ministeri Henrik Ramsayn kanssa ja toimi Suomen Punaisessa Ristissä.\" (from the corpus)\n",
    "\n",
    "Test documents are not lemmatized.\n",
    "\n",
    "Also short sentences (e.g. \"Hän sai nimityksen kenraaliksi\") were tested, but the results were sigificantly worse than for the long ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('27', 0.6963678002357483), ('22', 0.68153977394104), ('76', 0.6719655394554138), ('25', 0.6616845726966858), ('21', 0.6556556820869446), ('157', 0.6308171153068542), ('198', 0.6302379965782166), ('23', 0.624057948589325), ('189', 0.6208353042602539), ('68', 0.6095075607299805)]\n",
      "[('23', 0.13377512991428375), ('189', 0.08624720573425293), ('22', 0.08456331491470337), ('145', 0.05946550890803337), ('78', 0.05627712234854698), ('148', 0.03999495506286621), ('154', 0.02445557340979576), ('149', 0.023879773914813995), ('143', 0.017699720337986946), ('52', 0.014712880365550518)]\n",
      "[('22', 0.6840628385543823), ('192', 0.6476923227310181), ('23', 0.6463348269462585), ('315', 0.480753093957901), ('189', 0.4704722464084625), ('21', 0.46617254614830017), ('266', 0.4640052020549774), ('25', 0.4380340576171875), ('137', 0.43303754925727844), ('20', 0.43024328351020813)]\n"
     ]
    }
   ],
   "source": [
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "test_data = word_tokenize(\"Hän sai nimityksen kenraaliksi\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "#print(\"V1_infer\", v1)\n",
    "\n",
    "test_data_M = word_tokenize(\"Hän avioitui Pietarissa 2. toukokuuta 1892 venäläisen kenraalimajurin varakkaan perijättären Anastasia Arapovan kanssa.\".lower())\n",
    "v_m = model.infer_vector(test_data_M)\n",
    "\n",
    "test_data_L = word_tokenize(\"Lönnrot meni naimisiin vuonna 1849 kappalaisen tyttären Maria Piponiuksen kanssa ja lähti opettamaan kirjallisuutta.\".lower())\n",
    "v_l = model.infer_vector(test_data_L)\n",
    "\n",
    "test_data_N = word_tokenize(\"Tytär Karin solmi avioliiton ministeri Henrik Ramsayn kanssa ja toimi Suomen Punaisessa Ristissä.\".lower())\n",
    "v_n = model.infer_vector(test_data_N)\n",
    "\n",
    "# to find most similar doc using tags\n",
    "#similar_doc = model.docvecs.most_similar('2')\n",
    "#print(similar_doc)\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "#print(model.docvecs['1'])\n",
    "\n",
    "similar_doc1 = model.docvecs.most_similar([v1])\n",
    "similar_doc_v_m = model.docvecs.most_similar([v_m],topn=10)\n",
    "similar_doc_v_l = model.docvecs.most_similar([v_l],topn=10)\n",
    "similar_doc_v_n = model.docvecs.most_similar([v_n],topn=10)\n",
    "#print(similar_doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are similar for the 1st and the 3rd test documents, but the results for the 2nd differ from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('27', 0.6963678002357483), ('22', 0.68153977394104), ('76', 0.6719655394554138), ('25', 0.6616845726966858), ('21', 0.6556556820869446), ('157', 0.6308171153068542), ('198', 0.6302379965782166), ('23', 0.624057948589325), ('189', 0.6208353042602539), ('68', 0.6095075607299805)]\n",
      "[('23', 0.13377512991428375), ('189', 0.08624720573425293), ('22', 0.08456331491470337), ('145', 0.05946550890803337), ('78', 0.05627712234854698), ('148', 0.03999495506286621), ('154', 0.02445557340979576), ('149', 0.023879773914813995), ('143', 0.017699720337986946), ('52', 0.014712880365550518)]\n",
      "[('22', 0.6840628385543823), ('192', 0.6476923227310181), ('23', 0.6463348269462585), ('315', 0.480753093957901), ('189', 0.4704722464084625), ('21', 0.46617254614830017), ('266', 0.4640052020549774), ('25', 0.4380340576171875), ('137', 0.43303754925727844), ('20', 0.43024328351020813)]\n"
     ]
    }
   ],
   "source": [
    "print(similar_doc_v_m)\n",
    "print(similar_doc_v_l)\n",
    "print(similar_doc_v_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the similarities for particular words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('21', 0.8781343698501587), ('23', 0.7849961519241333), ('27', 0.7636508345603943), ('25', 0.7487219572067261), ('22', 0.7202984094619751), ('192', 0.7149435877799988), ('170', 0.6967310309410095), ('76', 0.6512880325317383), ('189', 0.6428390145301819), ('94', 0.6211383938789368)]\n",
      "[('350', 0.7831231355667114), ('23', 0.7823107242584229), ('180', 0.7546542882919312), ('316', 0.7542555332183838), ('189', 0.7463042736053467), ('22', 0.7123032808303833), ('145', 0.6880351305007935), ('299', 0.68767249584198), ('181', 0.6724562048912048), ('155', 0.6524287462234497)]\n",
      "[('22', 0.900201141834259), ('23', 0.7920703291893005), ('192', 0.7810046076774597), ('21', 0.7547167539596558), ('27', 0.7032747864723206), ('146', 0.694972813129425), ('14', 0.6924290060997009), ('181', 0.6843852400779724), ('137', 0.6786996126174927), ('315', 0.6674879789352417)]\n"
     ]
    }
   ],
   "source": [
    "print(model.docvecs.most_similar([model['avioitua']]))\n",
    "print(model.docvecs.most_similar([model['naimisiin']]))\n",
    "print(model.docvecs.most_similar([model['avioliitto']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem seems to be with 'naimisiin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
